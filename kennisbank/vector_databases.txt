VECTOR DATABASES - EMBEDDINGS - SIMILARITY SEARCH

=== WAT IS EEN VECTOR DATABASE? ===

Een vector database is een database geoptimaliseerd voor het opslaan en doorzoeken van vectoren (embeddings). Anders dan traditionele databases die zoeken op exacte waarden, vinden vector databases items die semantisch vergelijkbaar zijn.

Waarom vector databases:
- Traditionele DB: zoekt exacte matches ("machine learning")
- Vector DB: zoekt op betekenis (vindt ook "ML", "AI", "deep learning")

Keywords: vector database, embeddings, similarity search, semantisch zoeken

---

=== WAT ZIJN EMBEDDINGS? ===

Embeddings zijn numerieke representaties van data (tekst, afbeeldingen, audio) in een vector ruimte. Semantisch vergelijkbare items hebben vectoren die dicht bij elkaar liggen.

Hoe werken embeddings:
- Tekst/afbeelding gaat door een neural network
- Output is een vector met honderden/duizenden getallen
- Voorbeeld: "kat" -> [0.2, -0.5, 0.8, 0.1, ...]
- "poes" heeft een vergelijkbare vector

Embedding modellen:
- OpenAI text-embedding-ada-002 (1536 dimensies)
- Voyage AI (1024 dimensies)
- Sentence Transformers (384-768 dimensies)
- BERT embeddings

Keywords: embeddings, vectoren, numerieke representatie, embedding model

---

=== HOE WERKT COSINE SIMILARITY? ===

Cosine similarity meet de hoek tussen twee vectoren. Het is de meest gebruikte metric voor tekst embeddings.

De formule:
similarity = (A . B) / (||A|| x ||B||)

Waarbij:
- A . B = dot product van vectoren A en B
- ||A|| = lengte (magnitude) van vector A
- ||B|| = lengte van vector B

Resultaat:
- 1.0 = identiek (hoek = 0 graden)
- 0.0 = geen relatie (hoek = 90 graden)
- -1.0 = tegenovergesteld (hoek = 180 graden)

Voorbeeld:
- "hond" en "puppy" -> similarity ~0.85
- "hond" en "auto" -> similarity ~0.15

Keywords: cosine similarity, similarity score, vector vergelijking, hoek

---

=== ANDERE SIMILARITY METRICS ===

Euclidean Distance (L2):
- Meet de rechte lijn afstand
- Formule: sqrt(sum((ai - bi)^2))
- Kleiner = meer vergelijkbaar
- Goed voor: clustering, afbeeldingen

Dot Product:
- Som van element-wise vermenigvuldiging
- Formule: sum(ai * bi)
- Groter = meer vergelijkbaar
- Goed voor: genormaliseerde vectoren

Manhattan Distance (L1):
- Som van absolute verschillen
- Formule: sum(|ai - bi|)
- Minder gevoelig voor outliers

---

=== POPULAIRE VECTOR DATABASES ===

Pinecone:
- Fully managed cloud service
- Zeer schaalbaar (miljarden vectoren)
- Makkelijke API
- Prijs: per vector per maand

ChromaDB:
- Open source
- Lokaal of in cloud
- Python-native
- Gratis voor lokaal gebruik

Weaviate:
- Open source
- Ingebouwde vectorisatie
- GraphQL API
- Hybrid search (vector + keyword)

Milvus:
- Open source
- Zeer schaalbaar
- GPU ondersteuning
- Enterprise ready

Qdrant:
- Open source, Rust-based
- Filtering en payloads
- Snel en efficient

FAISS (Facebook):
- Library, geen volledige database
- Extreem snel
- Beste voor research

---

=== RAG MET VECTOR DATABASE ===

RAG (Retrieval-Augmented Generation) combineert vector databases met LLMs.

Indexeer fase:
1. Documenten splitsen in chunks
2. Elk chunk omzetten naar embedding
3. Embeddings opslaan in vector DB

Query fase:
1. Vraag omzetten naar embedding
2. Vector DB zoekt vergelijkbare chunks
3. Top-K chunks ophalen

Generatie fase:
1. Chunks toevoegen aan LLM prompt
2. LLM genereert antwoord met context
3. Antwoord is gebaseerd op je documenten

Keywords: RAG, retrieval augmented generation, chunks, indexeren
