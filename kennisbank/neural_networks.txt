NEURAL NETWORKS - NEURALE NETWERKEN - DEEP LEARNING

=== WAT IS EEN NEURAL NETWORK? ===

Een neural network (neuraal netwerk) is een machine learning model geinspireerd op het menselijk brein. Het bestaat uit lagen van "neuronen" die informatie verwerken en doorgeven.

Neural networks kunnen complexe patronen leren die traditionele algoritmes niet kunnen ontdekken.

Keywords: neural network, neuraal netwerk, deep learning, neuronen, lagen

---

=== STRUCTUUR VAN EEN NEURAL NETWORK ===

INPUT LAYER (Invoerlaag):
- Ontvangt de ruwe data
- Elk neuron = een feature
- Voorbeeld: 784 neuronen voor 28x28 afbeelding

HIDDEN LAYERS (Verborgen Lagen):
- Hier wordt geleerd
- Elke laag leert abstractere features
- Meer lagen = dieper netwerk = deep learning
- Laag 1: detecteert randen
- Laag 2: detecteert vormen
- Laag 3: detecteert objecten

OUTPUT LAYER (Uitvoerlaag):
- Geeft het eindresultaat
- Classificatie: een neuron per klasse
- Regressie: een neuron met getal

---

=== HOE WERKT BACKPROPAGATION? ===

Backpropagation is het algoritme waarmee neural networks leren. Het berekent hoe elke weight bijdraagt aan de fout en past deze aan.

Stap 1 - FORWARD PROPAGATION:
- Data stroomt van input naar output
- Elk neuron: output = activation(weights * inputs + bias)
- Netwerk maakt een voorspelling

Stap 2 - LOSS CALCULATION:
- Vergelijk voorspelling met correct antwoord
- Loss functie meet de fout
- Voorbeelden: Cross-Entropy Loss, MSE Loss

Stap 3 - BACKPROPAGATION:
- Bereken gradient van loss naar elke weight
- Gebruik chain rule (kettingregel)
- Gradients stromen TERUG door het netwerk
- Vandaar de naam "back" propagation

Stap 4 - WEIGHT UPDATE:
- Pas weights aan in richting die loss verlaagt
- Optimizer bepaalt hoe (SGD, Adam, RMSprop)
- Learning rate bepaalt stapgrootte

Keywords: backpropagation, gradient descent, loss functie, weights, training

---

=== ACTIVATION FUNCTIONS ===

ReLU (Rectified Linear Unit):
- Formule: f(x) = max(0, x)
- Meest populair voor hidden layers
- Snel en voorkomt vanishing gradients

Sigmoid:
- Formule: f(x) = 1 / (1 + e^-x)
- Output tussen 0 en 1
- Goed voor binaire classificatie output

Tanh:
- Formule: f(x) = (e^x - e^-x) / (e^x + e^-x)
- Output tussen -1 en 1
- Betere centering dan sigmoid

Softmax:
- Zet vector om naar kansen (som = 1)
- Gebruikt voor multi-class classificatie
- Laatste laag bij classificatie problemen

---

=== DEEP LEARNING ARCHITECTUREN ===

CNN (Convolutional Neural Networks):
- Speciaal voor beelden en spatiele data
- Convolutional layers detecteren lokale patronen
- Pooling layers verkleinen de data
- Toepassingen: beeldherkenning, object detectie

RNN (Recurrent Neural Networks):
- Voor sequentiele data (tekst, tijdreeksen)
- Hebben geheugen van vorige inputs
- Probleem: vanishing gradients bij lange sequenties

LSTM (Long Short-Term Memory):
- Verbeterde RNN met gates
- Kan lange termijn afhankelijkheden leren
- Gates: forget, input, output

Transformer:
- Revolutionair voor NLP
- Attention mechanisme weegt relevantie
- Basis voor GPT, BERT, Claude
- Kan parallel verwerken (sneller dan RNN)

---

=== TRAINING TIPS ===

- Begin met kleine learning rate (0.001)
- Gebruik batch normalization
- Dropout voorkomt overfitting (0.2-0.5)
- Data augmentation vergroot je dataset
- Monitor validation loss voor early stopping
- Gebruik Adam optimizer als startpunt
