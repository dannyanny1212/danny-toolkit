NEURAL NETWORKS - NEURALE NETWERKEN - DEEP LEARNING

=== WAT IS EEN NEURAL NETWORK? ===

Een neural network (neuraal netwerk) is een machine learning model geinspireerd op het menselijk brein. Het bestaat uit lagen van "neuronen" die informatie verwerken en doorgeven.

Neural networks kunnen complexe patronen leren die traditionele algoritmes niet kunnen ontdekken.

Keywords: neural network, neuraal netwerk, deep learning, neuronen, lagen

---

=== STRUCTUUR VAN EEN NEURAL NETWORK ===

INPUT LAYER (Invoerlaag):
- Ontvangt de ruwe data
- Elk neuron = een feature
- Voorbeeld: 784 neuronen voor 28x28 afbeelding

HIDDEN LAYERS (Verborgen Lagen):
- Hier wordt geleerd
- Elke laag leert abstractere features
- Meer lagen = dieper netwerk = deep learning
- Laag 1: detecteert randen
- Laag 2: detecteert vormen
- Laag 3: detecteert objecten

OUTPUT LAYER (Uitvoerlaag):
- Geeft het eindresultaat
- Classificatie: een neuron per klasse
- Regressie: een neuron met getal

---

=== HOE WERKT BACKPROPAGATION? ===

Backpropagation is het leeralgoritme voor neural networks. Het werkt in 4 stappen:

STAP 1 FORWARD PASS: Data stroomt van input naar output. Elk neuron berekent output = activation(weights * inputs + bias). Het netwerk maakt een voorspelling.

STAP 2 LOSS BEREKENEN: De loss functie meet de fout tussen voorspelling en correct antwoord. Lage loss = goede voorspelling. Voorbeelden: MSE Loss, Cross-Entropy Loss.

STAP 3 BACKWARD PASS: Bereken de gradient (afgeleide) van de loss naar elke weight. Dit gebeurt met de chain rule uit calculus. De gradients stromen terug door het netwerk, vandaar de naam "back" propagation.

STAP 4 WEIGHTS UPDATEN: Pas de weights aan met de formule: nieuwe_weight = oude_weight - learning_rate * gradient. Optimizers zoals SGD, Adam en RMSprop bepalen hoe dit precies gebeurt.

Dit proces herhaalt zich voor elke batch trainingsdata tot het netwerk convergeert (loss stopt met dalen).

Keywords: backpropagation, back propagation, gradient descent, chain rule, forward pass, backward pass, loss function, weights updaten

---

=== ACTIVATION FUNCTIONS ===

ReLU (Rectified Linear Unit):
- Formule: f(x) = max(0, x)
- Meest populair voor hidden layers
- Snel en voorkomt vanishing gradients

Sigmoid:
- Formule: f(x) = 1 / (1 + e^-x)
- Output tussen 0 en 1
- Goed voor binaire classificatie output

Tanh:
- Formule: f(x) = (e^x - e^-x) / (e^x + e^-x)
- Output tussen -1 en 1
- Betere centering dan sigmoid

Softmax:
- Zet vector om naar kansen (som = 1)
- Gebruikt voor multi-class classificatie
- Laatste laag bij classificatie problemen

---

=== WAT IS EEN CNN? ===

Een CNN (Convolutional Neural Network) is een type neural network speciaal ontworpen voor het verwerken van beelden en afbeeldingen. CNN staat voor Convolutional Neural Network.

Hoe werkt een CNN:
- Input: een afbeelding (bijvoorbeeld 224x224 pixels)
- Convolutional layers: detecteren features zoals randen, vormen
- Pooling layers: verkleinen de data, behouden belangrijkste info
- Fully connected layers: maken de uiteindelijke classificatie

Onderdelen van een CNN:

CONVOLUTIONAL LAYER:
- Past filters (kernels) toe op de afbeelding
- Filter schuift over de afbeelding
- Detecteert lokale patronen (randen, texturen)
- Elke filter detecteert een ander patroon

POOLING LAYER:
- Verkleint de afbeelding (downsample)
- Max pooling: neemt hoogste waarde in een gebied
- Vermindert aantal parameters
- Maakt netwerk robuuster

Toepassingen van CNN:
- Beeldherkenning (foto's classificeren)
- Object detectie (waar is de kat?)
- Gezichtsherkenning
- Medische beeldanalyse
- Zelfrijdende auto's

Keywords: CNN, Convolutional Neural Network, beeldherkenning, convolutie, pooling, afbeeldingen

---

=== WAT IS EEN RNN? ===

Een RNN (Recurrent Neural Network) is een neural network voor sequentiele data zoals tekst en tijdreeksen. RNN heeft een "geheugen" van vorige inputs.

Kenmerken van RNN:
- Verwerkt data stap voor stap
- Houdt een hidden state bij
- Output hangt af van vorige inputs
- Probleem: vanishing gradients bij lange sequenties

Keywords: RNN, Recurrent Neural Network, sequenties, geheugen, tijdreeksen

---

=== WAT IS EEN LSTM? ===

LSTM (Long Short-Term Memory) is een verbeterde versie van RNN die lange-termijn afhankelijkheden kan leren.

LSTM gates:
- Forget gate: wat vergeten uit geheugen
- Input gate: wat toevoegen aan geheugen
- Output gate: wat outputten

Keywords: LSTM, Long Short-Term Memory, gates, lange termijn

---

=== WAT IS EEN TRANSFORMER? ===

Transformer is een revolutionaire architectuur voor NLP (Natural Language Processing). Het is de basis voor GPT, BERT en Claude.

Kenmerken van Transformer:
- Attention mechanisme weegt relevantie van woorden
- Kan parallel verwerken (sneller dan RNN)
- Self-attention: elk woord kijkt naar alle andere woorden
- Zeer effectief voor taal taken

Keywords: Transformer, attention, self-attention, NLP, GPT, BERT

---

=== TRAINING TIPS ===

- Begin met kleine learning rate (0.001)
- Gebruik batch normalization
- Dropout voorkomt overfitting (0.2-0.5)
- Data augmentation vergroot je dataset
- Monitor validation loss voor early stopping
- Gebruik Adam optimizer als startpunt
